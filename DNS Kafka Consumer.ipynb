{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7f5cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # for data visualization purposes\n",
    "import seaborn as sns # for statistical data visualization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd2572c",
   "metadata": {},
   "source": [
    "# For first window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dc557cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(\n",
    "    'ml-raw-dns',\n",
    "    bootstrap_servers=\"localhost:9092\",\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=False\n",
    ")\n",
    "a=[]\n",
    "empty_Arr=[]\n",
    "i=0\n",
    "results=[]\n",
    "\n",
    "for m in consumer:\n",
    "    a.append(m)\n",
    "    if(i==1000):\n",
    "        break\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d7f596",
   "metadata": {},
   "source": [
    "## Datamaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96aad3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,1001):\n",
    "    demo=(a[i][6]).decode(\"utf-8\")\n",
    "    demo=demo[2:-3]\n",
    "    demo=demo.split(\",\")\n",
    "    empty_Arr.append(demo)\n",
    "df2=pd.DataFrame(columns=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric','entropy','special','labels','labels_max','labels_average','longest_word','sld','len','subdomain','Target Attack'])\n",
    "for i in range(0,len(empty_Arr)):\n",
    "    df2 = df2.append(pd.Series(empty_Arr[i], index=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric','entropy','special','labels','labels_max','labels_average','longest_word','sld','len','subdomain','Target Attack']), \n",
    "           ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fd5e81",
   "metadata": {},
   "source": [
    "## Feature extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc0d8506",
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = FeatureHasher(n_features=3, input_type='string')\n",
    "#for feature timestamp\n",
    "hashed1 = fh.transform(df2[['timestamp']].astype(str).values)\n",
    "hashed1 = pd.DataFrame(hashed1.todense())\n",
    "hashed1.columns = ['timestamp_fh'+str(i) for i in hashed1.columns]\n",
    "hashed2 = fh.transform(df2[['longest_word']].astype(str).values)\n",
    "hashed2 = pd.DataFrame(hashed2.todense())\n",
    "hashed2.columns = ['longest_word_fh'+str(i) for i in hashed2.columns]\n",
    "hashed3 = fh.transform(df2[['sld']].astype(str).values)\n",
    "hashed3 = pd.DataFrame(hashed3.todense())\n",
    "hashed3.columns = ['sld_fh'+str(i) for i in hashed3.columns]\n",
    "df_1 = pd.concat([df2,hashed1,hashed2,hashed3],axis=1)\n",
    "X =df_1.drop([\"timestamp\", \"longest_word\", \"sld\", \"Target Attack\"], axis = 1)\n",
    "y = df2[\"Target Attack\"]\n",
    "y=y.astype(float)\n",
    "#choose best features for training\n",
    "train_labels = X[['subdomain_length', 'lower', 'numeric', 'special', 'labels_max', 'len',\n",
    "       'longest_word_fh0', 'longest_word_fh1', 'sld_fh0', 'sld_fh1',\n",
    "       'sld_fh2']]\n",
    "train_labels = train_labels.astype(float)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_labels,y ,\n",
    "                                   random_state=104, \n",
    "                                   test_size=0.20, \n",
    "                                   shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4bc0ff",
   "metadata": {},
   "source": [
    "Accuracy from Static model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa5bd554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.829\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'xgboost_model.sav'\n",
    "# pickle.dump(xgb_clf, open(filename, 'wb'))\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(train_labels, y)\n",
    "results.append(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d15aa",
   "metadata": {},
   "source": [
    "Making a dynamic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bb73fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(learning_rate =0.01, n_estimators=5000, max_depth=17, min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.005, objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "xgb_pred = xgb_clf.predict(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f6cfcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, xgb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a5e8964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'xgboost_model_dynamic_window1.sav'\n",
    "pickle.dump(xgb_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3862d7f",
   "metadata": {},
   "source": [
    "# For second window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42e27e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.811\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "i=0\n",
    "empty_Arr=[]\n",
    "df2.drop(df2.index, inplace=True)\n",
    "for m in consumer:\n",
    "    if(i>1100 and i <= 2100):\n",
    "        a.append(m)\n",
    "    if(i==2100):\n",
    "        break\n",
    "    i=i+1\n",
    "empty_Arr=[]\n",
    "for i in range(0,1000):\n",
    "    demo=(a[i][6]).decode(\"utf-8\")\n",
    "    demo=demo[2:-3]\n",
    "    demo=demo.split(\",\")\n",
    "    empty_Arr.append(demo)\n",
    "df2=pd.DataFrame(columns=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric','entropy','special','labels','labels_max','labels_average','longest_word','sld','len','subdomain','Target Attack'])\n",
    "for i in range(0,len(empty_Arr)):\n",
    "    df2 = df2.append(pd.Series(empty_Arr[i], index=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric','entropy','special','labels','labels_max','labels_average','longest_word','sld','len','subdomain','Target Attack']), \n",
    "           ignore_index=True)\n",
    "fh = FeatureHasher(n_features=3, input_type='string')\n",
    "#for feature timestamp\n",
    "hashed1 = fh.transform(df2[['timestamp']].astype(str).values)\n",
    "hashed1 = pd.DataFrame(hashed1.todense())\n",
    "hashed1.columns = ['timestamp_fh'+str(i) for i in hashed1.columns]\n",
    "hashed2 = fh.transform(df2[['longest_word']].astype(str).values)\n",
    "hashed2 = pd.DataFrame(hashed2.todense())\n",
    "hashed2.columns = ['longest_word_fh'+str(i) for i in hashed2.columns]\n",
    "hashed3 = fh.transform(df2[['sld']].astype(str).values)\n",
    "hashed3 = pd.DataFrame(hashed3.todense())\n",
    "hashed3.columns = ['sld_fh'+str(i) for i in hashed3.columns]\n",
    "df_1 = pd.concat([df2,hashed1,hashed2,hashed3],axis=1)\n",
    "X =df_1.drop([\"timestamp\", \"longest_word\", \"sld\", \"Target Attack\"], axis = 1)\n",
    "y = df2[\"Target Attack\"]\n",
    "y=y.astype(float)\n",
    "#choose best features for training\n",
    "train_labels = X[['subdomain_length', 'lower', 'numeric', 'special', 'labels_max', 'len',\n",
    "       'longest_word_fh0', 'longest_word_fh1', 'sld_fh0', 'sld_fh1',\n",
    "       'sld_fh2']]\n",
    "train_labels = train_labels.astype(float)\n",
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'xgboost_model.sav'\n",
    "# pickle.dump(xgb_clf, open(filename, 'wb'))\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(train_labels, y)\n",
    "results.append(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "859170e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, xgb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0adf5fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.809\n"
     ]
    }
   ],
   "source": [
    "filename = 'xgboost_model_dynamic_window1.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(train_labels, y)\n",
    "results.append(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21739d9",
   "metadata": {},
   "source": [
    "# For third window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f86eed8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.797\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "i=0\n",
    "empty_Arr=[]\n",
    "df2.drop(df2.index, inplace=True)\n",
    "for m in consumer:\n",
    "    if(i>2100 and i <= 3100):\n",
    "        a.append(m)\n",
    "    if(i==3100):\n",
    "        break\n",
    "    i=i+1\n",
    "empty_Arr=[]\n",
    "for i in range(0,1000):\n",
    "    demo=(a[i][6]).decode(\"utf-8\")\n",
    "    demo=demo[2:-3]\n",
    "    demo=demo.split(\",\")\n",
    "    empty_Arr.append(demo)\n",
    "df2=pd.DataFrame(columns=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric','entropy','special','labels','labels_max','labels_average','longest_word','sld','len','subdomain','Target Attack'])\n",
    "for i in range(0,len(empty_Arr)):\n",
    "    df2 = df2.append(pd.Series(empty_Arr[i], index=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric','entropy','special','labels','labels_max','labels_average','longest_word','sld','len','subdomain','Target Attack']), \n",
    "           ignore_index=True)\n",
    "fh = FeatureHasher(n_features=3, input_type='string')\n",
    "#for feature timestamp\n",
    "hashed1 = fh.transform(df2[['timestamp']].astype(str).values)\n",
    "hashed1 = pd.DataFrame(hashed1.todense())\n",
    "hashed1.columns = ['timestamp_fh'+str(i) for i in hashed1.columns]\n",
    "hashed2 = fh.transform(df2[['longest_word']].astype(str).values)\n",
    "hashed2 = pd.DataFrame(hashed2.todense())\n",
    "hashed2.columns = ['longest_word_fh'+str(i) for i in hashed2.columns]\n",
    "hashed3 = fh.transform(df2[['sld']].astype(str).values)\n",
    "hashed3 = pd.DataFrame(hashed3.todense())\n",
    "hashed3.columns = ['sld_fh'+str(i) for i in hashed3.columns]\n",
    "df_1 = pd.concat([df2,hashed1,hashed2,hashed3],axis=1)\n",
    "X =df_1.drop([\"timestamp\", \"longest_word\", \"sld\", \"Target Attack\"], axis = 1)\n",
    "y = df2[\"Target Attack\"]\n",
    "y=y.astype(float)\n",
    "#choose best features for training\n",
    "train_labels = X[['subdomain_length', 'lower', 'numeric', 'special', 'labels_max', 'len',\n",
    "       'longest_word_fh0', 'longest_word_fh1', 'sld_fh0', 'sld_fh1',\n",
    "       'sld_fh2']]\n",
    "train_labels = train_labels.astype(float)\n",
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'xgboost_model.sav'\n",
    "# pickle.dump(xgb_clf, open(filename, 'wb'))\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(train_labels, y)\n",
    "results.append(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b98d4a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.798\n"
     ]
    }
   ],
   "source": [
    "filename = 'xgboost_model_dynamic_window1.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(train_labels, y)\n",
    "results.append(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480395dc",
   "metadata": {},
   "source": [
    "# Fourth window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97d0b1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.813\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "i=0\n",
    "empty_Arr=[]\n",
    "df2.drop(df2.index, inplace=True)\n",
    "for m in consumer:\n",
    "    if(i>3100 and i <= 4100):\n",
    "        a.append(m)\n",
    "    if(i==4100):\n",
    "        break\n",
    "    i=i+1\n",
    "empty_Arr=[]\n",
    "for i in range(0,1000):\n",
    "    demo=(a[i][6]).decode(\"utf-8\")\n",
    "    demo=demo[2:-3]\n",
    "    demo=demo.split(\",\")\n",
    "    empty_Arr.append(demo)\n",
    "df2=pd.DataFrame(columns=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric','entropy','special','labels','labels_max','labels_average','longest_word','sld','len','subdomain','Target Attack'])\n",
    "for i in range(0,len(empty_Arr)):\n",
    "    df2 = df2.append(pd.Series(empty_Arr[i], index=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric','entropy','special','labels','labels_max','labels_average','longest_word','sld','len','subdomain','Target Attack']), \n",
    "           ignore_index=True)\n",
    "fh = FeatureHasher(n_features=3, input_type='string')\n",
    "#for feature timestamp\n",
    "hashed1 = fh.transform(df2[['timestamp']].astype(str).values)\n",
    "hashed1 = pd.DataFrame(hashed1.todense())\n",
    "hashed1.columns = ['timestamp_fh'+str(i) for i in hashed1.columns]\n",
    "hashed2 = fh.transform(df2[['longest_word']].astype(str).values)\n",
    "hashed2 = pd.DataFrame(hashed2.todense())\n",
    "hashed2.columns = ['longest_word_fh'+str(i) for i in hashed2.columns]\n",
    "hashed3 = fh.transform(df2[['sld']].astype(str).values)\n",
    "hashed3 = pd.DataFrame(hashed3.todense())\n",
    "hashed3.columns = ['sld_fh'+str(i) for i in hashed3.columns]\n",
    "df_1 = pd.concat([df2,hashed1,hashed2,hashed3],axis=1)\n",
    "X =df_1.drop([\"timestamp\", \"longest_word\", \"sld\", \"Target Attack\"], axis = 1)\n",
    "y = df2[\"Target Attack\"]\n",
    "y=y.astype(float)\n",
    "#choose best features for training\n",
    "train_labels = X[['subdomain_length', 'lower', 'numeric', 'special', 'labels_max', 'len',\n",
    "       'longest_word_fh0', 'longest_word_fh1', 'sld_fh0', 'sld_fh1',\n",
    "       'sld_fh2']]\n",
    "train_labels = train_labels.astype(float)\n",
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'xgboost_model.sav'\n",
    "# pickle.dump(xgb_clf, open(filename, 'wb'))\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(train_labels, y)\n",
    "results.append(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20833ce9",
   "metadata": {},
   "source": [
    "# Fifth window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8a0e695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.825\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "i=0\n",
    "empty_Arr=[]\n",
    "df2.drop(df2.index, inplace=True)\n",
    "for m in consumer:\n",
    "    if(i>4100 and i <= 5100):\n",
    "        a.append(m)\n",
    "    if(i==5100):\n",
    "        break\n",
    "    i=i+1\n",
    "empty_Arr=[]\n",
    "for i in range(0,1000):\n",
    "    demo=(a[i][6]).decode(\"utf-8\")\n",
    "    demo=demo[2:-3]\n",
    "    demo=demo.split(\",\")\n",
    "    empty_Arr.append(demo)\n",
    "df2=pd.DataFrame(columns=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric','entropy','special','labels','labels_max','labels_average','longest_word','sld','len','subdomain','Target Attack'])\n",
    "for i in range(0,len(empty_Arr)):\n",
    "    df2 = df2.append(pd.Series(empty_Arr[i], index=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric','entropy','special','labels','labels_max','labels_average','longest_word','sld','len','subdomain','Target Attack']), \n",
    "           ignore_index=True)\n",
    "fh = FeatureHasher(n_features=3, input_type='string')\n",
    "#for feature timestamp\n",
    "hashed1 = fh.transform(df2[['timestamp']].astype(str).values)\n",
    "hashed1 = pd.DataFrame(hashed1.todense())\n",
    "hashed1.columns = ['timestamp_fh'+str(i) for i in hashed1.columns]\n",
    "hashed2 = fh.transform(df2[['longest_word']].astype(str).values)\n",
    "hashed2 = pd.DataFrame(hashed2.todense())\n",
    "hashed2.columns = ['longest_word_fh'+str(i) for i in hashed2.columns]\n",
    "hashed3 = fh.transform(df2[['sld']].astype(str).values)\n",
    "hashed3 = pd.DataFrame(hashed3.todense())\n",
    "hashed3.columns = ['sld_fh'+str(i) for i in hashed3.columns]\n",
    "df_1 = pd.concat([df2,hashed1,hashed2,hashed3],axis=1)\n",
    "X =df_1.drop([\"timestamp\", \"longest_word\", \"sld\", \"Target Attack\"], axis = 1)\n",
    "y = df2[\"Target Attack\"]\n",
    "y=y.astype(float)\n",
    "#choose best features for training\n",
    "train_labels = X[['subdomain_length', 'lower', 'numeric', 'special', 'labels_max', 'len',\n",
    "       'longest_word_fh0', 'longest_word_fh1', 'sld_fh0', 'sld_fh1',\n",
    "       'sld_fh2']]\n",
    "train_labels = train_labels.astype(float)\n",
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'xgboost_model.sav'\n",
    "# pickle.dump(xgb_clf, open(filename, 'wb'))\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(train_labels, y)\n",
    "results.append(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3327cc3",
   "metadata": {},
   "source": [
    "# Sixth window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "534efbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.818\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "i=0\n",
    "empty_Arr=[]\n",
    "df2.drop(df2.index, inplace=True)\n",
    "for m in consumer:\n",
    "    if(i>5100 and i <= 6100):\n",
    "        a.append(m)\n",
    "    if(i==6100):\n",
    "        break\n",
    "    i=i+1\n",
    "empty_Arr=[]\n",
    "for i in range(0,1000):\n",
    "    demo=(a[i][6]).decode(\"utf-8\")\n",
    "    demo=demo[2:-3]\n",
    "    demo=demo.split(\",\")\n",
    "    empty_Arr.append(demo)\n",
    "df2=pd.DataFrame(columns=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric','entropy','special','labels','labels_max','labels_average','longest_word','sld','len','subdomain','Target Attack'])\n",
    "for i in range(0,len(empty_Arr)):\n",
    "    df2 = df2.append(pd.Series(empty_Arr[i], index=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric','entropy','special','labels','labels_max','labels_average','longest_word','sld','len','subdomain','Target Attack']), \n",
    "           ignore_index=True)\n",
    "fh = FeatureHasher(n_features=3, input_type='string')\n",
    "#for feature timestamp\n",
    "hashed1 = fh.transform(df2[['timestamp']].astype(str).values)\n",
    "hashed1 = pd.DataFrame(hashed1.todense())\n",
    "hashed1.columns = ['timestamp_fh'+str(i) for i in hashed1.columns]\n",
    "hashed2 = fh.transform(df2[['longest_word']].astype(str).values)\n",
    "hashed2 = pd.DataFrame(hashed2.todense())\n",
    "hashed2.columns = ['longest_word_fh'+str(i) for i in hashed2.columns]\n",
    "hashed3 = fh.transform(df2[['sld']].astype(str).values)\n",
    "hashed3 = pd.DataFrame(hashed3.todense())\n",
    "hashed3.columns = ['sld_fh'+str(i) for i in hashed3.columns]\n",
    "df_1 = pd.concat([df2,hashed1,hashed2,hashed3],axis=1)\n",
    "X =df_1.drop([\"timestamp\", \"longest_word\", \"sld\", \"Target Attack\"], axis = 1)\n",
    "y = df2[\"Target Attack\"]\n",
    "y=y.astype(float)\n",
    "#choose best features for training\n",
    "train_labels = X[['subdomain_length', 'lower', 'numeric', 'special', 'labels_max', 'len',\n",
    "       'longest_word_fh0', 'longest_word_fh1', 'sld_fh0', 'sld_fh1',\n",
    "       'sld_fh2']]\n",
    "train_labels = train_labels.astype(float)\n",
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'xgboost_model.sav'\n",
    "# pickle.dump(xgb_clf, open(filename, 'wb'))\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(train_labels, y)\n",
    "results.append(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a6e2fa",
   "metadata": {},
   "source": [
    "# seventh window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a74a21d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.829\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "i=0\n",
    "empty_Arr=[]\n",
    "df2.drop(df2.index, inplace=True)\n",
    "for m in consumer:\n",
    "    if(i>6100 and i <= 7100):\n",
    "        a.append(m)\n",
    "    if(i==7100):\n",
    "        break\n",
    "    i=i+1\n",
    "empty_Arr=[]\n",
    "for i in range(0,1000):\n",
    "    demo=(a[i][6]).decode(\"utf-8\")\n",
    "    demo=demo[2:-3]\n",
    "    demo=demo.split(\",\")\n",
    "    empty_Arr.append(demo)\n",
    "df2=pd.DataFrame(columns=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric','entropy','special','labels','labels_max','labels_average','longest_word','sld','len','subdomain','Target Attack'])\n",
    "for i in range(0,len(empty_Arr)):\n",
    "    df2 = df2.append(pd.Series(empty_Arr[i], index=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric','entropy','special','labels','labels_max','labels_average','longest_word','sld','len','subdomain','Target Attack']), \n",
    "           ignore_index=True)\n",
    "fh = FeatureHasher(n_features=3, input_type='string')\n",
    "#for feature timestamp\n",
    "hashed1 = fh.transform(df2[['timestamp']].astype(str).values)\n",
    "hashed1 = pd.DataFrame(hashed1.todense())\n",
    "hashed1.columns = ['timestamp_fh'+str(i) for i in hashed1.columns]\n",
    "hashed2 = fh.transform(df2[['longest_word']].astype(str).values)\n",
    "hashed2 = pd.DataFrame(hashed2.todense())\n",
    "hashed2.columns = ['longest_word_fh'+str(i) for i in hashed2.columns]\n",
    "hashed3 = fh.transform(df2[['sld']].astype(str).values)\n",
    "hashed3 = pd.DataFrame(hashed3.todense())\n",
    "hashed3.columns = ['sld_fh'+str(i) for i in hashed3.columns]\n",
    "df_1 = pd.concat([df2,hashed1,hashed2,hashed3],axis=1)\n",
    "X =df_1.drop([\"timestamp\", \"longest_word\", \"sld\", \"Target Attack\"], axis = 1)\n",
    "y = df2[\"Target Attack\"]\n",
    "y=y.astype(float)\n",
    "#choose best features for training\n",
    "train_labels = X[['subdomain_length', 'lower', 'numeric', 'special', 'labels_max', 'len',\n",
    "       'longest_word_fh0', 'longest_word_fh1', 'sld_fh0', 'sld_fh1',\n",
    "       'sld_fh2']]\n",
    "train_labels = train_labels.astype(float)\n",
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'xgboost_model.sav'\n",
    "# pickle.dump(xgb_clf, open(filename, 'wb'))\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(train_labels, y)\n",
    "results.append(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9539e000",
   "metadata": {},
   "source": [
    "# 8th window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ca6abba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "i=0\n",
    "empty_Arr=[]\n",
    "df2.drop(df2.index, inplace=True)\n",
    "for m in consumer:\n",
    "    if(i>7100 and i <= 8100):\n",
    "        a.append(m)\n",
    "    if(i==8100):\n",
    "        break\n",
    "    i=i+1\n",
    "empty_Arr=[]\n",
    "for i in range(0,1000):\n",
    "    demo=(a[i][6]).decode(\"utf-8\")\n",
    "    demo=demo[2:-3]\n",
    "    demo=demo.split(\",\")\n",
    "    empty_Arr.append(demo)\n",
    "df2=pd.DataFrame(columns=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric','entropy','special','labels','labels_max','labels_average','longest_word','sld','len','subdomain','Target Attack'])\n",
    "for i in range(0,len(empty_Arr)):\n",
    "    df2 = df2.append(pd.Series(empty_Arr[i], index=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric','entropy','special','labels','labels_max','labels_average','longest_word','sld','len','subdomain','Target Attack']), \n",
    "           ignore_index=True)\n",
    "fh = FeatureHasher(n_features=3, input_type='string')\n",
    "#for feature timestamp\n",
    "hashed1 = fh.transform(df2[['timestamp']].astype(str).values)\n",
    "hashed1 = pd.DataFrame(hashed1.todense())\n",
    "hashed1.columns = ['timestamp_fh'+str(i) for i in hashed1.columns]\n",
    "hashed2 = fh.transform(df2[['longest_word']].astype(str).values)\n",
    "hashed2 = pd.DataFrame(hashed2.todense())\n",
    "hashed2.columns = ['longest_word_fh'+str(i) for i in hashed2.columns]\n",
    "hashed3 = fh.transform(df2[['sld']].astype(str).values)\n",
    "hashed3 = pd.DataFrame(hashed3.todense())\n",
    "hashed3.columns = ['sld_fh'+str(i) for i in hashed3.columns]\n",
    "df_1 = pd.concat([df2,hashed1,hashed2,hashed3],axis=1)\n",
    "X =df_1.drop([\"timestamp\", \"longest_word\", \"sld\", \"Target Attack\"], axis = 1)\n",
    "y = df2[\"Target Attack\"]\n",
    "y=y.astype(float)\n",
    "#choose best features for training\n",
    "train_labels = X[['subdomain_length', 'lower', 'numeric', 'special', 'labels_max', 'len',\n",
    "       'longest_word_fh0', 'longest_word_fh1', 'sld_fh0', 'sld_fh1',\n",
    "       'sld_fh2']]\n",
    "train_labels = train_labels.astype(float)\n",
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'xgboost_model.sav'\n",
    "# pickle.dump(xgb_clf, open(filename, 'wb'))\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(train_labels, y)\n",
    "results.append(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68575954",
   "metadata": {},
   "source": [
    "# Ninth window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf75d977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.832\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "i=0\n",
    "empty_Arr=[]\n",
    "df2.drop(df2.index, inplace=True)\n",
    "for m in consumer:\n",
    "    if(i>8100 and i <= 9100):\n",
    "        a.append(m)\n",
    "    if(i==9100):\n",
    "        break\n",
    "    i=i+1\n",
    "empty_Arr=[]\n",
    "for i in range(0,1000):\n",
    "    demo=(a[i][6]).decode(\"utf-8\")\n",
    "    demo=demo[2:-3]\n",
    "    demo=demo.split(\",\")\n",
    "    empty_Arr.append(demo)\n",
    "df2=pd.DataFrame(columns=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric','entropy','special','labels','labels_max','labels_average','longest_word','sld','len','subdomain','Target Attack'])\n",
    "for i in range(0,len(empty_Arr)):\n",
    "    df2 = df2.append(pd.Series(empty_Arr[i], index=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric','entropy','special','labels','labels_max','labels_average','longest_word','sld','len','subdomain','Target Attack']), \n",
    "           ignore_index=True)\n",
    "fh = FeatureHasher(n_features=3, input_type='string')\n",
    "#for feature timestamp\n",
    "hashed1 = fh.transform(df2[['timestamp']].astype(str).values)\n",
    "hashed1 = pd.DataFrame(hashed1.todense())\n",
    "hashed1.columns = ['timestamp_fh'+str(i) for i in hashed1.columns]\n",
    "hashed2 = fh.transform(df2[['longest_word']].astype(str).values)\n",
    "hashed2 = pd.DataFrame(hashed2.todense())\n",
    "hashed2.columns = ['longest_word_fh'+str(i) for i in hashed2.columns]\n",
    "hashed3 = fh.transform(df2[['sld']].astype(str).values)\n",
    "hashed3 = pd.DataFrame(hashed3.todense())\n",
    "hashed3.columns = ['sld_fh'+str(i) for i in hashed3.columns]\n",
    "df_1 = pd.concat([df2,hashed1,hashed2,hashed3],axis=1)\n",
    "X =df_1.drop([\"timestamp\", \"longest_word\", \"sld\", \"Target Attack\"], axis = 1)\n",
    "y = df2[\"Target Attack\"]\n",
    "y=y.astype(float)\n",
    "#choose best features for training\n",
    "train_labels = X[['subdomain_length', 'lower', 'numeric', 'special', 'labels_max', 'len',\n",
    "       'longest_word_fh0', 'longest_word_fh1', 'sld_fh0', 'sld_fh1',\n",
    "       'sld_fh2']]\n",
    "train_labels = train_labels.astype(float)\n",
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'xgboost_model.sav'\n",
    "# pickle.dump(xgb_clf, open(filename, 'wb'))\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(train_labels, y)\n",
    "results.append(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7f8620",
   "metadata": {},
   "source": [
    "# Tenth window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aba68077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.824\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "i=0\n",
    "empty_Arr=[]\n",
    "df2.drop(df2.index, inplace=True)\n",
    "for m in consumer:\n",
    "    if(i>9100 and i <= 10100):\n",
    "        a.append(m)\n",
    "    if(i==10100):\n",
    "        break\n",
    "    i=i+1\n",
    "empty_Arr=[]\n",
    "for i in range(0,1000):\n",
    "    demo=(a[i][6]).decode(\"utf-8\")\n",
    "    demo=demo[2:-3]\n",
    "    demo=demo.split(\",\")\n",
    "    empty_Arr.append(demo)\n",
    "df2=pd.DataFrame(columns=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric','entropy','special','labels','labels_max','labels_average','longest_word','sld','len','subdomain','Target Attack'])\n",
    "for i in range(0,len(empty_Arr)):\n",
    "    df2 = df2.append(pd.Series(empty_Arr[i], index=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric','entropy','special','labels','labels_max','labels_average','longest_word','sld','len','subdomain','Target Attack']), \n",
    "           ignore_index=True)\n",
    "fh = FeatureHasher(n_features=3, input_type='string')\n",
    "#for feature timestamp\n",
    "hashed1 = fh.transform(df2[['timestamp']].astype(str).values)\n",
    "hashed1 = pd.DataFrame(hashed1.todense())\n",
    "hashed1.columns = ['timestamp_fh'+str(i) for i in hashed1.columns]\n",
    "hashed2 = fh.transform(df2[['longest_word']].astype(str).values)\n",
    "hashed2 = pd.DataFrame(hashed2.todense())\n",
    "hashed2.columns = ['longest_word_fh'+str(i) for i in hashed2.columns]\n",
    "hashed3 = fh.transform(df2[['sld']].astype(str).values)\n",
    "hashed3 = pd.DataFrame(hashed3.todense())\n",
    "hashed3.columns = ['sld_fh'+str(i) for i in hashed3.columns]\n",
    "df_1 = pd.concat([df2,hashed1,hashed2,hashed3],axis=1)\n",
    "X =df_1.drop([\"timestamp\", \"longest_word\", \"sld\", \"Target Attack\"], axis = 1)\n",
    "y = df2[\"Target Attack\"]\n",
    "y=y.astype(float)\n",
    "#choose best features for training\n",
    "train_labels = X[['subdomain_length', 'lower', 'numeric', 'special', 'labels_max', 'len',\n",
    "       'longest_word_fh0', 'longest_word_fh1', 'sld_fh0', 'sld_fh1',\n",
    "       'sld_fh2']]\n",
    "train_labels = train_labels.astype(float)\n",
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'xgboost_model.sav'\n",
    "# pickle.dump(xgb_clf, open(filename, 'wb'))\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(train_labels, y)\n",
    "results.append(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "659915dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.829,\n",
       " 0.811,\n",
       " 0.809,\n",
       " 0.797,\n",
       " 0.798,\n",
       " 0.813,\n",
       " 0.825,\n",
       " 0.818,\n",
       " 0.829,\n",
       " 0.83,\n",
       " 0.832,\n",
       " 0.824]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "855c96c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-e0c0c1ec3cbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# X, y = make_blobs(n_samples=1000, centers=2, n_features=2, random_state=1, cluster_std=3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# define bounds of the domain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# generate dataset\n",
    "# X, y = make_blobs(n_samples=1000, centers=2, n_features=2, random_state=1, cluster_std=3)\n",
    "X= train_labels.to_numpy()\n",
    "X = X.iloc[:, :2].values\n",
    "y=y.to_numpy()\n",
    "# define bounds of the domain\n",
    "min1, max1 = X[:, 0].min()-1, X[:, 0].max()+1\n",
    "min2, max2 = X[:, 1].min()-1, X[:, 1].max()+1\n",
    "# define the x and y scale\n",
    "x1grid = np.arange(min1, max1, 0.1)\n",
    "x2grid = np.arange(min2, max2, 0.1)\n",
    "# create all of the lines and rows of the grid\n",
    "xx, yy = np.meshgrid(x1grid, x2grid)\n",
    "# flatten each grid to a vector\n",
    "r1, r2 = xx.flatten(), yy.flatten()\n",
    "r1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\n",
    "# horizontal stack vectors to create x1,x2 input for the model\n",
    "grid = np.hstack((r1,r2))\n",
    "# define the model\n",
    "model = LogisticRegression()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# make predictions for the grid\n",
    "yhat = model.predict(grid)\n",
    "# reshape the predictions back into a grid\n",
    "zz = yhat.reshape(xx.shape)\n",
    "# plot the grid of x, y and z values as a surface\n",
    "plt.contourf(xx, yy, zz, cmap='Paired')\n",
    "# create scatter plot for samples from each class\n",
    "for class_value in range(2):\n",
    "    # get row indexes for samples with this class\n",
    "    row_ix = np.where(y == class_value)\n",
    "    # create scatter of these samples\n",
    "    plt.scatter(X[row_ix, 0], X[row_ix, 1], cmap='Paired')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724971d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9a6704b",
   "metadata": {},
   "source": [
    "# Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8aa2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.DataFrame(columns=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric','entropy','special','labels','labels_max','labels_average','longest_word','sld','len','subdomain','Target Attack'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605b25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for itr in range(1,11):\n",
    "    a=[]\n",
    "    i=0\n",
    "    empty_Arr=[]\n",
    "    df2.drop(df2.index, inplace=True)\n",
    "    for m in consumer:\n",
    "        if(i>(itr*1000)+100 and i <= (itr*1000)+100):\n",
    "            a.append(m)\n",
    "        if(i==(itr*1000)+100):\n",
    "            break\n",
    "        i=i+1\n",
    "    empty_Arr=[]\n",
    "    for i in range(0,1000):\n",
    "        demo=(a[i][6]).decode(\"utf-8\")\n",
    "        demo=demo[2:-3]\n",
    "        demo=demo.split(\",\")\n",
    "        empty_Arr.append(demo)\n",
    "    df2=pd.DataFrame(columns=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric','entropy','special','labels','labels_max','labels_average','longest_word','sld','len','subdomain','Target Attack'])\n",
    "    for i in range(0,len(empty_Arr)):\n",
    "        df2 = df2.append(pd.Series(empty_Arr[i], index=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric','entropy','special','labels','labels_max','labels_average','longest_word','sld','len','subdomain','Target Attack']), \n",
    "            ignore_index=True)\n",
    "    fh = FeatureHasher(n_features=3, input_type='string')\n",
    "    #for feature timestamp\n",
    "    hashed1 = fh.transform(df2[['timestamp']].astype(str).values)\n",
    "    hashed1 = pd.DataFrame(hashed1.todense())\n",
    "    hashed1.columns = ['timestamp_fh'+str(i) for i in hashed1.columns]\n",
    "    hashed2 = fh.transform(df2[['longest_word']].astype(str).values)\n",
    "    hashed2 = pd.DataFrame(hashed2.todense())\n",
    "    hashed2.columns = ['longest_word_fh'+str(i) for i in hashed2.columns]\n",
    "    hashed3 = fh.transform(df2[['sld']].astype(str).values)\n",
    "    hashed3 = pd.DataFrame(hashed3.todense())\n",
    "    hashed3.columns = ['sld_fh'+str(i) for i in hashed3.columns]\n",
    "    df_1 = pd.concat([df2,hashed1,hashed2,hashed3],axis=1)\n",
    "    X =df_1.drop([\"timestamp\", \"longest_word\", \"sld\", \"Target Attack\"], axis = 1)\n",
    "    y = df2[\"Target Attack\"]\n",
    "    y=y.astype(float)\n",
    "    #choose best features for training\n",
    "    train_labels = X[['subdomain_length', 'lower', 'numeric', 'special', 'labels_max', 'len',\n",
    "        'longest_word_fh0', 'longest_word_fh1', 'sld_fh0', 'sld_fh1',\n",
    "        'sld_fh2']]\n",
    "    train_labels = train_labels.astype(float)\n",
    "    import pickle\n",
    "    # save the model to disk\n",
    "    filename = 'xgboost_model.sav'\n",
    "    # pickle.dump(xgb_clf, open(filename, 'wb'))\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    result = loaded_model.score(train_labels, y)\n",
    "    results.append(result)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36b43243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8325"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(learning_rate =0.001, n_estimators=1500)\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "xgb_clf.score(X_train,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19efa8f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "87d9b9d1e678527538d8142a74b02950316a4861c664997bcf14b93b840e5ec0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
